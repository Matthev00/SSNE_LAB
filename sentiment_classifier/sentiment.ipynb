{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "import wandb\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Set seed for full reproducibility in Python, NumPy, PyTorch (CPU & GPU) and CUDNN.\n",
    "\n",
    "    Args:\n",
    "        seed (int): Random seed to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 256\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "HIDDEN_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/hate_train.csv\")\n",
    "df[\"sentence\"] = df[\"sentence\"].str.replace(r\"@anonymized_account\", \"\", regex=True).str.strip()\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepsense-ai/trelbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(150)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = df[\"sentence\"].str.len().max()\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, text_col=\"sentence\", label_col=\"label\", max_length=256):\n",
    "        self.texts = df[text_col].tolist()\n",
    "        self.labels = df[label_col].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.float32)  \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SentimentDataset(train_df, tokenizer, max_length=MAX_LENGTH)\n",
    "val_dataset = SentimentDataset(val_df, tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos = train_df[\"label\"].value_counts().sort_index().values\n",
    "pos_weight = torch.tensor([neg / pos], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 256])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(batch[\"input_ids\"].shape)   \n",
    "print(batch[\"label\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size: int = 32, max_length: int = 256):\n",
    "    df = pd.read_csv(\"data/hate_train.csv\")\n",
    "    df[\"sentence\"] = df[\"sentence\"].str.replace(r\"@anonymized_account\", \"\", regex=True).str.strip()\n",
    "\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "    neg, pos = train_df[\"label\"].value_counts().sort_index().values\n",
    "    pos_weight = torch.tensor([neg / pos], dtype=torch.float32).to(DEVICE)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"deepsense-ai/trelbert\")\n",
    "    train_dataset = SentimentDataset(train_df, tokenizer, max_length=max_length)\n",
    "    val_dataset = SentimentDataset(val_df, tokenizer, max_length=max_length)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    return train_loader, val_loader, pos_weight\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrelBERTClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size: int = 256, dropout: float = 0.2, freeze_encoder: bool = False):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(\"deepsense-ai/trelbert\")\n",
    "        if freeze_encoder:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.encoder.config.hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        self.freeze_encoder = freeze_encoder\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_token = output.last_hidden_state[:, 0]\n",
    "        return self.classifier(cls_token)\n",
    "\n",
    "    def save(self, path: Path):\n",
    "        if self.freeze_encoder:\n",
    "            torch.save(self.classifier.state_dict(), path)\n",
    "        else:\n",
    "            torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load(self, path: Path):\n",
    "        if self.freeze_encoder:\n",
    "            self.classifier.load_state_dict(torch.load(path, map_location=\"cpu\"))\n",
    "        else:\n",
    "            self.load_state_dict(torch.load(path, map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifierTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        criterion: nn.Module,\n",
    "        device: torch.device,\n",
    "        run_name: str,\n",
    "        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,\n",
    "        save_dir: Path = Path(\"checkpoints/\"),\n",
    "        max_epochs: int = 50,\n",
    "        log_wandb: bool = True,\n",
    "    ) -> None:\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.save_dir = save_dir\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.max_epochs = max_epochs\n",
    "        self.run_name = run_name\n",
    "        self.log_wandb = log_wandb\n",
    "\n",
    "        self.best_val_f1 = 0\n",
    "        self.best_model_path = self.save_dir / f\"{run_name}_best.pt\"\n",
    "\n",
    "        if self.log_wandb:\n",
    "            wandb.watch(self.model)\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in tqdm(range(1, self.max_epochs + 1)):\n",
    "            train_loss = self._train_one_epoch()\n",
    "            val_loss, val_metrics = self._validate()\n",
    "\n",
    "            if self.scheduler:\n",
    "                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    self.scheduler.step(val_loss)\n",
    "                else:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            if self.log_wandb:\n",
    "                wandb.log({\n",
    "                    \"epoch\": epoch,\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    **{f\"val_{k}\": v for k, v in val_metrics.items()},\n",
    "                    \"learning_rate\": self.optimizer.param_groups[0][\"lr\"]\n",
    "                })\n",
    "\n",
    "            if val_metrics[\"f1\"] > self.best_val_f1:\n",
    "                self.best_val_f1 = val_metrics[\"f1\"]\n",
    "                self.model.save(self.best_model_path)\n",
    "\n",
    "        print(f\"âœ… Best model saved at: {self.best_model_path}\")\n",
    "\n",
    "    def _train_one_epoch(self) -> float:\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch in self.train_loader:\n",
    "            X = batch[\"input_ids\"].to(self.device)\n",
    "            mask = batch[\"attention_mask\"].to(self.device)\n",
    "            y = batch[\"label\"].to(self.device).float()\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            logits = self.model(X, attention_mask=mask).view(-1)\n",
    "            loss = self.criterion(logits, y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * X.size(0)\n",
    "\n",
    "        return total_loss / len(self.train_loader.dataset)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def _validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "\n",
    "        for batch in self.val_loader:\n",
    "            X = batch[\"input_ids\"].to(self.device)\n",
    "            mask = batch[\"attention_mask\"].to(self.device)\n",
    "            y = batch[\"label\"].to(self.device).float()\n",
    "            \n",
    "            logits = self.model(X, attention_mask=mask).view(-1)\n",
    "            loss = self.criterion(logits, y)\n",
    "            total_loss += loss.item() * X.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).long()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(y.cpu().numpy())\n",
    "\n",
    "        avg_loss = total_loss / len(self.val_loader.dataset)\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(all_targets, all_preds),\n",
    "            \"f1\": f1_score(all_targets, all_preds),\n",
    "            \"precision\": precision_score(all_targets, all_preds),\n",
    "            \"recall\": recall_score(all_targets, all_preds),\n",
    "        }\n",
    "        return avg_loss, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_train():\n",
    "    wandb.init()\n",
    "    set_seed(42)\n",
    "    config = wandb.config\n",
    "\n",
    "    train_loader, val_loader, pos_weight = get_dataloaders(batch_size=config.batch_size, max_length=config.max_length)\n",
    "\n",
    "    model = TrelBERTClassifier(\n",
    "        hidden_size=config.hidden_size,\n",
    "        dropout=config.dropout,\n",
    "        freeze_encoder=config.freeze_encoder,\n",
    "    )\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=config.lr,\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", patience=2\n",
    "    )\n",
    "\n",
    "    trainer = BinaryClassifierTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=loss_fn,\n",
    "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        run_name=wandb.run.name,\n",
    "        scheduler=scheduler,\n",
    "        save_dir=Path(\"checkpoints\"),\n",
    "        max_epochs=config.max_epochs,\n",
    "        log_wandb=True\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "sweep_config = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\"name\": \"val_f1\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"hidden_size\": {\"values\": [128, 256]},\n",
    "        \"max_length\": {\"values\": [128, 256]},\n",
    "        \"dropout\": {\"values\": [0.1, 0.2]},\n",
    "        \"freeze_encoder\": {\"values\": [True]},\n",
    "        \"batch_size\": {\"values\": [16, 32]},\n",
    "        \"lr\": {\"min\": 1e-5, \"max\": 5e-4},\n",
    "        \"weight_decay\": {\"values\": [0.0, 1e-5]},\n",
    "        \"max_epochs\": {\"value\": 10},\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: eubcd3ki\n",
      "Sweep URL: https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/sweeps/eubcd3ki\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: o0arfj4t with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_encoder: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 5.25256435196723e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatthev00\u001b[0m (\u001b[33mMY_EXPERIMENTS\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'SSNE-sentiment' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mateusz/PW/SSNE/lab/sentiment_classifier/wandb/run-20250604_175725-o0arfj4t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/runs/o0arfj4t' target=\"_blank\">dashing-sweep-1</a></strong> to <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/sweeps/eubcd3ki' target=\"_blank\">https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/sweeps/eubcd3ki</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment' target=\"_blank\">https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/sweeps/eubcd3ki' target=\"_blank\">https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/sweeps/eubcd3ki</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/runs/o0arfj4t' target=\"_blank\">https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/runs/o0arfj4t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at deepsense-ai/trelbert and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dashing-sweep-1</strong> at: <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/runs/o0arfj4t' target=\"_blank\">https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/runs/o0arfj4t</a><br> View project at: <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment' target=\"_blank\">https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250604_175725-o0arfj4t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run o0arfj4t errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mateusz/PW/SSNE/lab/.venv/lib/python3.13/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "    ~~~~~~~~~~~~~~^^\n",
      "  File \"/media/mateusz/DATA/tmp/ipykernel_243031/2336353267.py\", line 39, in sweep_train\n",
      "    trainer.train()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"/media/mateusz/DATA/tmp/ipykernel_243031/800989751.py\", line 37, in train\n",
      "    train_loss = self._train_one_epoch()\n",
      "  File \"/media/mateusz/DATA/tmp/ipykernel_243031/800989751.py\", line 72, in _train_one_epoch\n",
      "    loss = self.criterion(logits, y)\n",
      "  File \"/home/mateusz/PW/SSNE/lab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mateusz/PW/SSNE/lab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/mateusz/PW/SSNE/lab/.venv/lib/python3.13/site-packages/torch/nn/modules/loss.py\", line 821, in forward\n",
      "    return F.binary_cross_entropy_with_logits(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        input,\n",
      "        ^^^^^^\n",
      "    ...<3 lines>...\n",
      "        reduction=self.reduction,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/mateusz/PW/SSNE/lab/.venv/lib/python3.13/site-packages/torch/nn/functional.py\", line 3643, in binary_cross_entropy_with_logits\n",
      "    return torch.binary_cross_entropy_with_logits(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        input, target, weight, pos_weight, reduction_enum\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "RuntimeError: result type Float can't be cast to the desired output type Long\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run o0arfj4t errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/mateusz/PW/SSNE/lab/.venv/lib/python3.13/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ~~~~~~~~~~~~~~^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/media/mateusz/DATA/tmp/ipykernel_243031/2336353267.py\", line 39, in sweep_train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.train()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ~~~~~~~~~~~~~^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/media/mateusz/DATA/tmp/ipykernel_243031/800989751.py\", line 37, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_loss = self._train_one_epoch()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/media/mateusz/DATA/tmp/ipykernel_243031/800989751.py\", line 72, in _train_one_epoch\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss = self.criterion(logits, y)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/mateusz/PW/SSNE/lab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/mateusz/PW/SSNE/lab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/mateusz/PW/SSNE/lab/.venv/lib/python3.13/site-packages/torch/nn/modules/loss.py\", line 821, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.binary_cross_entropy_with_logits(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         input,\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ...<3 lines>...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         reduction=self.reduction,\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     )\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/mateusz/PW/SSNE/lab/.venv/lib/python3.13/site-packages/torch/nn/functional.py\", line 3643, in binary_cross_entropy_with_logits\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.binary_cross_entropy_with_logits(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         input, target, weight, pos_weight, reduction_enum\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     )\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m RuntimeError: result type Float can't be cast to the desired output type Long\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x13fktnh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_encoder: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0004181745755768582\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 1e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'SSNE-sentiment' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mateusz/PW/SSNE/lab/sentiment_classifier/wandb/run-20250604_180131-x13fktnh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/runs/x13fktnh' target=\"_blank\">leafy-sweep-2</a></strong> to <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/sweeps/eubcd3ki' target=\"_blank\">https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/sweeps/eubcd3ki</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment' target=\"_blank\">https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/sweeps/eubcd3ki' target=\"_blank\">https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/sweeps/eubcd3ki</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/runs/x13fktnh' target=\"_blank\">https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/runs/x13fktnh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at deepsense-ai/trelbert and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">leafy-sweep-2</strong> at: <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/runs/x13fktnh' target=\"_blank\">https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/runs/x13fktnh</a><br> View project at: <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment' target=\"_blank\">https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250604_180131-x13fktnh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run x13fktnh errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mateusz/PW/SSNE/lab/.venv/lib/python3.13/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "    ~~~~~~~~~~~~~~^^\n",
      "  File \"/media/mateusz/DATA/tmp/ipykernel_243031/2336353267.py\", line 39, in sweep_train\n",
      "    trainer.train()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"/media/mateusz/DATA/tmp/ipykernel_243031/800989751.py\", line 37, in train\n",
      "    train_loss = self._train_one_epoch()\n",
      "  File \"/media/mateusz/DATA/tmp/ipykernel_243031/800989751.py\", line 72, in _train_one_epoch\n",
      "    loss = self.criterion(logits, y)\n",
      "  File \"/home/mateusz/PW/SSNE/lab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mateusz/PW/SSNE/lab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/mateusz/PW/SSNE/lab/.venv/lib/python3.13/site-packages/torch/nn/modules/loss.py\", line 821, in forward\n",
      "    return F.binary_cross_entropy_with_logits(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        input,\n",
      "        ^^^^^^\n",
      "    ...<3 lines>...\n",
      "        reduction=self.reduction,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/mateusz/PW/SSNE/lab/.venv/lib/python3.13/site-packages/torch/nn/functional.py\", line 3643, in binary_cross_entropy_with_logits\n",
      "    return torch.binary_cross_entropy_with_logits(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        input, target, weight, pos_weight, reduction_enum\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "RuntimeError: result type Float can't be cast to the desired output type Long\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run x13fktnh errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/mateusz/PW/SSNE/lab/.venv/lib/python3.13/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ~~~~~~~~~~~~~~^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/media/mateusz/DATA/tmp/ipykernel_243031/2336353267.py\", line 39, in sweep_train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.train()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ~~~~~~~~~~~~~^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/media/mateusz/DATA/tmp/ipykernel_243031/800989751.py\", line 37, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_loss = self._train_one_epoch()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/media/mateusz/DATA/tmp/ipykernel_243031/800989751.py\", line 72, in _train_one_epoch\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss = self.criterion(logits, y)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/mateusz/PW/SSNE/lab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/mateusz/PW/SSNE/lab/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/mateusz/PW/SSNE/lab/.venv/lib/python3.13/site-packages/torch/nn/modules/loss.py\", line 821, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.binary_cross_entropy_with_logits(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         input,\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ...<3 lines>...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         reduction=self.reduction,\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     )\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/mateusz/PW/SSNE/lab/.venv/lib/python3.13/site-packages/torch/nn/functional.py\", line 3643, in binary_cross_entropy_with_logits\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.binary_cross_entropy_with_logits(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         input, target, weight, pos_weight, reduction_enum\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     )\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m RuntimeError: result type Float can't be cast to the desired output type Long\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: v101575v with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_encoder: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0003494910968181072\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 1e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'SSNE-sentiment' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mateusz/PW/SSNE/lab/sentiment_classifier/wandb/run-20250604_180141-v101575v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/runs/v101575v' target=\"_blank\">magic-sweep-3</a></strong> to <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/sweeps/eubcd3ki' target=\"_blank\">https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/sweeps/eubcd3ki</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment' target=\"_blank\">https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/sweeps/eubcd3ki' target=\"_blank\">https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/sweeps/eubcd3ki</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/runs/v101575v' target=\"_blank\">https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/runs/v101575v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at deepsense-ai/trelbert and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">magic-sweep-3</strong> at: <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/runs/v101575v' target=\"_blank\">https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/runs/v101575v</a><br> View project at: <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment' target=\"_blank\">https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250604_180141-v101575v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(\n",
    "    sweep_config, project=\"SSNE-sentiment\", entity=\"MY_EXPERIMENTS\"\n",
    ")\n",
    "wandb.agent(sweep_id, function=sweep_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
