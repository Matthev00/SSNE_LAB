{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hOOZAXk0sDoe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from torch import nn\n",
        "import wandb\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LRkqXjAhsDog"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UcKs0LxRsDoh"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed: int) -> None:\n",
        "    \"\"\"\n",
        "    Set seed for full reproducibility in Python, NumPy, PyTorch (CPU & GPU) and CUDNN.\n",
        "\n",
        "    Args:\n",
        "        seed (int): Random seed to set.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aVxQowPYsDoi"
      },
      "outputs": [],
      "source": [
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, text_col=\"sentence\", label_col=\"label\", max_length=256):\n",
        "        self.texts = df[text_col].tolist()\n",
        "        self.labels = df[label_col].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoded = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
        "            \"label\": torch.tensor(label, dtype=torch.float32)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7brbPh38sDol"
      },
      "outputs": [],
      "source": [
        "def get_dataloaders(batch_size: int = 32, max_length: int = 256):\n",
        "    df = pd.read_csv(\"hate_train.csv\")\n",
        "    df[\"sentence\"] = df[\"sentence\"].str.strip()\n",
        "\n",
        "    train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
        "    neg, pos = train_df[\"label\"].value_counts().sort_index().values\n",
        "    pos_weight = torch.tensor([neg / pos], dtype=torch.float32).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"deepsense-ai/trelbert\")\n",
        "    train_dataset = SentimentDataset(train_df, tokenizer, max_length=max_length)\n",
        "    val_dataset = SentimentDataset(val_df, tokenizer, max_length=max_length)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    return train_loader, val_loader, pos_weight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "k2vXhmyrsDom"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "class TrelBERTClassifier(nn.Module):\n",
        "    def __init__(self, dropout: float = 0.2, freeze_encoder: bool = False):\n",
        "        super().__init__()\n",
        "        config = AutoConfig.from_pretrained(\n",
        "            \"deepsense-ai/trelbert\",\n",
        "            num_labels=1,\n",
        "            problem_type=\"single_label_classification\",\n",
        "            hidden_dropout_prob=dropout\n",
        "        )\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            \"deepsense-ai/trelbert\",\n",
        "            config=config\n",
        "        )\n",
        "        self.freeze_encoder = freeze_encoder\n",
        "\n",
        "        if freeze_encoder:\n",
        "            for name, param in self.model.base_model.named_parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        return self.model(input_ids=input_ids, attention_mask=attention_mask).logits.squeeze(-1)\n",
        "\n",
        "    def save(self, path: Path):\n",
        "        torch.save(self.state_dict(), path)\n",
        "\n",
        "    def load(self, path: Path):\n",
        "        self.load_state_dict(torch.load(path, map_location=\"cpu\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z_6GhO6ysDop"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "\n",
        "class BinaryClassifierTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        train_loader: DataLoader,\n",
        "        val_loader: DataLoader,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        criterion: nn.Module,\n",
        "        device: torch.device,\n",
        "        run_name: str,\n",
        "        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,\n",
        "        save_dir: Path = Path(\"checkpoints/\"),\n",
        "        max_epochs: int = 50,\n",
        "        log_wandb: bool = True,\n",
        "        sigmoid_threshold: float = 0.3\n",
        "    ) -> None:\n",
        "        self.model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.criterion = criterion\n",
        "        self.device = device\n",
        "        self.save_dir = save_dir\n",
        "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.max_epochs = max_epochs\n",
        "        self.run_name = run_name\n",
        "        self.log_wandb = log_wandb\n",
        "        self.sigmoid_threshold = sigmoid_threshold\n",
        "\n",
        "        self.best_val_f1 = 0\n",
        "        self.best_model_path = self.save_dir / f\"{run_name}_best.pt\"\n",
        "\n",
        "        if self.log_wandb:\n",
        "            wandb.watch(self.model)\n",
        "\n",
        "    def train(self):\n",
        "        for epoch in tqdm(range(1, self.max_epochs + 1)):\n",
        "            train_loss = self._train_one_epoch()\n",
        "            val_loss, val_metrics = self._validate()\n",
        "\n",
        "            if self.scheduler:\n",
        "                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                    self.scheduler.step(val_loss)\n",
        "                else:\n",
        "                    self.scheduler.step()\n",
        "\n",
        "            if self.log_wandb:\n",
        "                wandb.log({\n",
        "                    \"epoch\": epoch,\n",
        "                    \"train_loss\": train_loss,\n",
        "                    \"val_loss\": val_loss,\n",
        "                    **{f\"val_{k}\": v for k, v in val_metrics.items()},\n",
        "                    \"learning_rate\": self.optimizer.param_groups[0][\"lr\"]\n",
        "                })\n",
        "\n",
        "            if val_metrics[\"f1\"] > self.best_val_f1:\n",
        "                self.best_val_f1 = val_metrics[\"f1\"]\n",
        "                self.model.save(self.best_model_path)\n",
        "\n",
        "    def _train_one_epoch(self) -> float:\n",
        "        self.model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for batch in self.train_loader:\n",
        "            X = batch[\"input_ids\"].to(self.device)\n",
        "            mask = batch[\"attention_mask\"].to(self.device)\n",
        "            y = batch[\"label\"].to(self.device).float()\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            logits = self.model(X, attention_mask=mask).view(-1)\n",
        "            loss = self.criterion(logits, y)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * X.size(0)\n",
        "\n",
        "        return total_loss / len(self.train_loader.dataset)\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def _validate(self):\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "\n",
        "        for batch in self.val_loader:\n",
        "            X = batch[\"input_ids\"].to(self.device)\n",
        "            mask = batch[\"attention_mask\"].to(self.device)\n",
        "            y = batch[\"label\"].to(self.device).float()\n",
        "\n",
        "            logits = self.model(X, attention_mask=mask).view(-1)\n",
        "            loss = self.criterion(logits, y)\n",
        "            total_loss += loss.item() * X.size(0)\n",
        "\n",
        "            probs = torch.sigmoid(logits)\n",
        "            preds = (probs > self.sigmoid_threshold).long()\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(y.cpu().numpy())\n",
        "\n",
        "        avg_loss = total_loss / len(self.val_loader.dataset)\n",
        "        metrics = {\n",
        "            \"accuracy\": accuracy_score(all_targets, all_preds),\n",
        "            \"f1\": f1_score(all_targets, all_preds),\n",
        "            \"precision\": precision_score(all_targets, all_preds),\n",
        "            \"recall\": recall_score(all_targets, all_preds),\n",
        "        }\n",
        "        return avg_loss, metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wHxbxd6QsDoq"
      },
      "outputs": [],
      "source": [
        "def sweep_train():\n",
        "    wandb.init()\n",
        "    set_seed(42)\n",
        "    config = wandb.config\n",
        "\n",
        "    train_loader, val_loader, pos_weight = get_dataloaders(batch_size=config.batch_size, max_length=config.max_length)\n",
        "\n",
        "    model = TrelBERTClassifier(\n",
        "        dropout=config.dropout,\n",
        "        freeze_encoder=config.freeze_encoder,\n",
        "    )\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()),\n",
        "        lr=config.lr,\n",
        "        weight_decay=config.weight_decay\n",
        "    )\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode=\"min\", patience=2\n",
        "    )\n",
        "\n",
        "    trainer = BinaryClassifierTrainer(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        optimizer=optimizer,\n",
        "        criterion=loss_fn,\n",
        "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        run_name=wandb.run.name,\n",
        "        scheduler=scheduler,\n",
        "        save_dir=Path(\"checkpoints\"),\n",
        "        max_epochs=config.max_epochs,\n",
        "        log_wandb=True,\n",
        "        sigmoid_threshold=config.sigmoid_threshold\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "\n",
        "sweep_config = {\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\"name\": \"val_f1\", \"goal\": \"maximize\"},\n",
        "    \"parameters\": {\n",
        "        \"max_length\": {\"values\": [128, 256]},\n",
        "        \"dropout\": {\"values\": [0.2, 0.3, 0.4]},\n",
        "        \"freeze_encoder\": {\"values\": [False, True]},\n",
        "        \"batch_size\": {\"values\": [256]},\n",
        "        \"lr\": {\"min\": 1e-5, \"max\": 5e-3},\n",
        "        \"weight_decay\": {\"values\": [1e-3, 1e-5]},\n",
        "        \"max_epochs\": {\"value\": 10},\n",
        "        \"sigmoid_threshold\": {\"value\": 0.5}\n",
        "    },\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.20.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (24.2)\n",
            "Requirement already satisfied: platformdirs in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (4.23.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (7.0.0)\n",
            "Requirement already satisfied: pydantic<3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (2.11.5)\n",
            "Requirement already satisfied: pyyaml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (2.29.1)\n",
            "Requirement already satisfied: setproctitle in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "\u001b[H\u001b[2J"
          ]
        }
      ],
      "source": [
        "!pip install wandb -U\n",
        "!clear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatthev00\u001b[0m (\u001b[33mMY_EXPERIMENTS\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Z02IAGUfsDoq",
        "outputId": "af4c77d7-fcb2-4eea-c219-e56ea69b8f1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: b7q0o7kd\n",
            "Sweep URL: https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/sweeps/b7q0o7kd\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7f6l8a7q with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfreeze_encoder: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0003507439042465243\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsigmoid_threshold: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 1e-05\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20250607_111955-7f6l8a7q</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/runs/7f6l8a7q' target=\"_blank\">spring-sweep-1</a></strong> to <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/sweeps/b7q0o7kd' target=\"_blank\">https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/sweeps/b7q0o7kd</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment' target=\"_blank\">https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/sweeps/b7q0o7kd' target=\"_blank\">https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/sweeps/b7q0o7kd</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/runs/7f6l8a7q' target=\"_blank\">https://wandb.ai/MY_EXPERIMENTS/SSNE-sentiment/runs/7f6l8a7q</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepsense-ai/trelbert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            " 10%|█         | 1/10 [02:25<21:51, 145.69s/it]"
          ]
        }
      ],
      "source": [
        "sweep_id = wandb.sweep(\n",
        "    sweep_config, project=\"SSNE-sentiment\", entity=\"MY_EXPERIMENTS\"\n",
        ")\n",
        "wandb.agent(sweep_id, function=sweep_train, count=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DM1tWKiY2V6b"
      },
      "outputs": [],
      "source": [
        "def run_inference(model_path: Path, input_txt: Path, output_csv: Path, max_length: int = 256):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"deepsense-ai/trelbert\")\n",
        "    model = TrelBERTClassifier()\n",
        "    model.load(model_path)\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    with open(input_txt, \"r\", encoding=\"utf-8\") as f:\n",
        "        texts = [line.strip() for line in f]\n",
        "\n",
        "    encodings = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    input_ids = encodings[\"input_ids\"].to(device)\n",
        "    attention_mask = encodings[\"attention_mask\"].to(device)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        logits = model(input_ids, attention_mask=attention_mask).view(-1)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = (probs > 0.5).long().cpu().tolist()\n",
        "\n",
        "    df_out = pd.DataFrame(preds)\n",
        "    df_out.to_csv(output_csv, index=False, header=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepsense-ai/trelbert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "run_inference(\"checkpoints/stilted-sweep-1_best.pt\", \"hate_test_data.txt\", \"pred.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
